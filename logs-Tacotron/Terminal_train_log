
-----------------------------------------------------------------
Starting new Tacotron training run
-----------------------------------------------------------------
[2020-03-10 23:20:53.664]  Checkpoint path: logs-Tacotron/taco_pretrained/tacotron_model.ckpt
[2020-03-10 23:20:54.134]  Loading training data from: dataset/training_data/train.txt
[2020-03-10 23:20:54.583]  Using model: Tacotron
[2020-03-10 23:20:55.293]  Hyperparameters:
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: (31,)
  cin_channels: 20
  cleaners: english_cleaners
  clip_mels_length: True
  cross_entropy_pos_weight: 20
  cumulative_weights: True
  decoder_layers: 2
  decoder_lstm_units: 1024
  embedding_dim: 512
  enc_conv_channels: 512
  enc_conv_kernel_size: (5,)
  enc_conv_num_layers: 3
  encoder_lstm_units: 256
  fmax: 7600
  fmin: 25
  frame_shift_ms: None
  freq_axis_kernel_size: 3
  gate_channels: 512
  gin_channels: -1
  griffin_lim_iters: 30
  hop_size: 128
  input_type: raw
  kernel_size: 3
  layers: 24
  log_scale_min: -32.23619130191664
  mask_decoder: True
  mask_encoder: True
  max_abs_value: 4.0
  max_iters: 1000
  max_mel_frames: 900
  max_time_sec: None
  max_time_steps: 13000
  min_level_db: -100
  n_fft: 512
  natural_eval: False
  num_freq: 513
  num_gpus: 1
  num_mels: 20
  out_channels: 30
  outputs_per_step: 2
  postnet_channels: 512
  postnet_kernel_size: (5,)
  postnet_num_layers: 5
  power: 1.2
  predict_linear: False
  prenet_layers: [256, 256]
  quantize_channels: 65536
  ref_level_db: 20
  rescale: True
  rescaling_max: 0.999
  residual_channels: 512
  sample_rate: 16000
  signal_normalization: True
  silence_threshold: 2
  skip_out_channels: 256
  smoothing: False
  stacks: 4
  stop_at_any: True
  symmetric_mels: True
  tacotron_adam_beta1: 0.9
  tacotron_adam_beta2: 0.999
  tacotron_adam_epsilon: 1e-06
  tacotron_batch_size: 32
  tacotron_data_random_state: 1234
  tacotron_decay_learning_rate: True
  tacotron_decay_rate: 0.5
  tacotron_decay_steps: 18000
  tacotron_dropout_rate: 0.5
  tacotron_final_learning_rate: 0.0001
  tacotron_initial_learning_rate: 0.001
  tacotron_random_seed: 5339
  tacotron_reg_weight: 1e-06
  tacotron_scale_regularization: True
  tacotron_start_decay: 40000
  tacotron_swap_with_cpu: False
  tacotron_teacher_forcing_decay_alpha: 0.0
  tacotron_teacher_forcing_decay_steps: 40000
  tacotron_teacher_forcing_final_ratio: 0.0
  tacotron_teacher_forcing_init_ratio: 1.0
  tacotron_teacher_forcing_mode: constant
  tacotron_teacher_forcing_ratio: 1.0
  tacotron_teacher_forcing_start_decay: 10000
  tacotron_test_batches: 16
  tacotron_test_size: None
  tacotron_zoneout_rate: 0.1
  train_with_GTA: False
  trim_fft_size: 128
  trim_hop_size: 64
  trim_silence: True
  trim_top_db: 40
  upsample_conditional_features: True
  upsample_scales: [16, 16]
  use_all_gpus: False
  use_bias: True
  use_lws: True
  wavenet_adam_beta1: 0.9
  wavenet_adam_beta2: 0.999
  wavenet_adam_epsilon: 1e-06
  wavenet_batch_size: 4
  wavenet_data_random_state: 1234
  wavenet_dropout: 0.05
  wavenet_ema_decay: 0.9999
  wavenet_learning_rate: 0.0001
  wavenet_random_seed: 5339
  wavenet_swap_with_cpu: False
  wavenet_test_batches: None
  wavenet_test_size: 0.0441
  win_size: 512
[2020-03-10 23:22:14.854]  Loaded metadata for 512 examples (0.43 hours)

-----------------------------------------------------------------
Starting new Tacotron training run
-----------------------------------------------------------------
[2020-03-10 23:26:58.112]  Checkpoint path: logs-Tacotron/taco_pretrained/tacotron_model.ckpt
[2020-03-10 23:26:58.113]  Loading training data from: dataset/training_data/train.txt
[2020-03-10 23:26:58.113]  Using model: Tacotron
[2020-03-10 23:26:58.113]  Hyperparameters:
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: (31,)
  cin_channels: 20
  cleaners: english_cleaners
  clip_mels_length: True
  cross_entropy_pos_weight: 20
  cumulative_weights: True
  decoder_layers: 2
  decoder_lstm_units: 1024
  embedding_dim: 512
  enc_conv_channels: 512
  enc_conv_kernel_size: (5,)
  enc_conv_num_layers: 3
  encoder_lstm_units: 256
  fmax: 7600
  fmin: 25
  frame_shift_ms: None
  freq_axis_kernel_size: 3
  gate_channels: 512
  gin_channels: -1
  griffin_lim_iters: 30
  hop_size: 128
  input_type: raw
  kernel_size: 3
  layers: 24
  log_scale_min: -32.23619130191664
  mask_decoder: True
  mask_encoder: True
  max_abs_value: 4.0
  max_iters: 1000
  max_mel_frames: 900
  max_time_sec: None
  max_time_steps: 13000
  min_level_db: -100
  n_fft: 512
  natural_eval: False
  num_freq: 513
  num_gpus: 1
  num_mels: 20
  out_channels: 30
  outputs_per_step: 2
  postnet_channels: 512
  postnet_kernel_size: (5,)
  postnet_num_layers: 5
  power: 1.2
  predict_linear: False
  prenet_layers: [256, 256]
  quantize_channels: 65536
  ref_level_db: 20
  rescale: True
  rescaling_max: 0.999
  residual_channels: 512
  sample_rate: 16000
  signal_normalization: True
  silence_threshold: 2
  skip_out_channels: 256
  smoothing: False
  stacks: 4
  stop_at_any: True
  symmetric_mels: True
  tacotron_adam_beta1: 0.9
  tacotron_adam_beta2: 0.999
  tacotron_adam_epsilon: 1e-06
  tacotron_batch_size: 8
  tacotron_data_random_state: 1234
  tacotron_decay_learning_rate: True
  tacotron_decay_rate: 0.5
  tacotron_decay_steps: 18000
  tacotron_dropout_rate: 0.5
  tacotron_final_learning_rate: 0.0001
  tacotron_initial_learning_rate: 0.001
  tacotron_random_seed: 5339
  tacotron_reg_weight: 1e-06
  tacotron_scale_regularization: True
  tacotron_start_decay: 40000
  tacotron_swap_with_cpu: False
  tacotron_teacher_forcing_decay_alpha: 0.0
  tacotron_teacher_forcing_decay_steps: 40000
  tacotron_teacher_forcing_final_ratio: 0.0
  tacotron_teacher_forcing_init_ratio: 1.0
  tacotron_teacher_forcing_mode: constant
  tacotron_teacher_forcing_ratio: 1.0
  tacotron_teacher_forcing_start_decay: 10000
  tacotron_test_batches: 8
  tacotron_test_size: None
  tacotron_zoneout_rate: 0.1
  train_with_GTA: False
  trim_fft_size: 128
  trim_hop_size: 64
  trim_silence: True
  trim_top_db: 40
  upsample_conditional_features: True
  upsample_scales: [16, 16]
  use_all_gpus: False
  use_bias: True
  use_lws: True
  wavenet_adam_beta1: 0.9
  wavenet_adam_beta2: 0.999
  wavenet_adam_epsilon: 1e-06
  wavenet_batch_size: 4
  wavenet_data_random_state: 1234
  wavenet_dropout: 0.05
  wavenet_ema_decay: 0.9999
  wavenet_learning_rate: 0.0001
  wavenet_random_seed: 5339
  wavenet_swap_with_cpu: False
  wavenet_test_batches: None
  wavenet_test_size: 0.0441
  win_size: 512
[2020-03-10 23:27:14.248]  Loaded metadata for 512 examples (0.43 hours)
[2020-03-10 23:30:11.915]  Initialized Tacotron model. Dimensions (? = dynamic shape): 
[2020-03-10 23:30:11.915]    Train mode:               True
[2020-03-10 23:30:11.916]    Eval mode:                False
[2020-03-10 23:30:11.916]    GTA mode:                 False
[2020-03-10 23:30:11.916]    Synthesis mode:           False
[2020-03-10 23:30:11.916]    embedding:                (?, ?, 512)
[2020-03-10 23:30:11.916]    enc conv out:             (?, ?, 512)
[2020-03-10 23:30:11.916]    encoder out:              (?, ?, 512)
[2020-03-10 23:30:11.916]    decoder out:              (?, ?, 20)
[2020-03-10 23:30:11.916]    residual out:             (?, ?, 512)
[2020-03-10 23:30:11.916]    projected residual out:   (?, ?, 20)
[2020-03-10 23:30:11.916]    mel out:                  (?, ?, 20)
[2020-03-10 23:30:11.916]    <stop_token> out:         (?, ?)
[2020-03-10 23:30:38.568]  Initialized Tacotron model. Dimensions (? = dynamic shape): 
[2020-03-10 23:30:38.568]    Train mode:               False
[2020-03-10 23:30:38.568]    Eval mode:                True
[2020-03-10 23:30:38.568]    GTA mode:                 False
[2020-03-10 23:30:38.568]    Synthesis mode:           False
[2020-03-10 23:30:38.568]    embedding:                (?, ?, 512)
[2020-03-10 23:30:38.568]    enc conv out:             (?, ?, 512)
[2020-03-10 23:30:38.569]    encoder out:              (?, ?, 512)
[2020-03-10 23:30:38.569]    decoder out:              (?, ?, 20)
[2020-03-10 23:30:38.569]    residual out:             (?, ?, 512)
[2020-03-10 23:30:38.569]    projected residual out:   (?, ?, 20)
[2020-03-10 23:30:38.569]    mel out:                  (?, ?, 20)
[2020-03-10 23:30:38.569]    <stop_token> out:         (?, ?)
[2020-03-10 23:30:53.557]  Tacotron training set to a maximum of 500000 steps
[2020-03-10 23:32:37.202]  No model to load at logs-Tacotron/taco_pretrained/
[2020-03-10 23:32:40.011]  
Generated 8 test batches of size 8 in 0.177 sec

-----------------------------------------------------------------
Starting new Tacotron training run
-----------------------------------------------------------------
[2020-03-10 23:44:22.023]  Checkpoint path: logs-Tacotron/taco_pretrained/tacotron_model.ckpt
[2020-03-10 23:44:22.023]  Loading training data from: dataset/training_data/train.txt
[2020-03-10 23:44:22.023]  Using model: Tacotron
[2020-03-10 23:44:22.023]  Hyperparameters:
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: (31,)
  cin_channels: 20
  cleaners: english_cleaners
  clip_mels_length: True
  cross_entropy_pos_weight: 20
  cumulative_weights: True
  decoder_layers: 2
  decoder_lstm_units: 1024
  embedding_dim: 512
  enc_conv_channels: 512
  enc_conv_kernel_size: (5,)
  enc_conv_num_layers: 3
  encoder_lstm_units: 256
  fmax: 7600
  fmin: 25
  frame_shift_ms: None
  freq_axis_kernel_size: 3
  gate_channels: 512
  gin_channels: -1
  griffin_lim_iters: 30
  hop_size: 128
  input_type: raw
  kernel_size: 3
  layers: 24
  log_scale_min: -32.23619130191664
  mask_decoder: True
  mask_encoder: True
  max_abs_value: 4.0
  max_iters: 1000
  max_mel_frames: 900
  max_time_sec: None
  max_time_steps: 13000
  min_level_db: -100
  n_fft: 512
  natural_eval: False
  num_freq: 257
  num_gpus: 1
  num_mels: 20
  out_channels: 30
  outputs_per_step: 2
  postnet_channels: 512
  postnet_kernel_size: (5,)
  postnet_num_layers: 5
  power: 1.2
  predict_linear: False
  prenet_layers: [256, 256]
  quantize_channels: 65536
  ref_level_db: 20
  rescale: True
  rescaling_max: 0.999
  residual_channels: 512
  sample_rate: 16000
  signal_normalization: True
  silence_threshold: 2
  skip_out_channels: 256
  smoothing: False
  stacks: 4
  stop_at_any: True
  symmetric_mels: True
  tacotron_adam_beta1: 0.9
  tacotron_adam_beta2: 0.999
  tacotron_adam_epsilon: 1e-06
  tacotron_batch_size: 8
  tacotron_data_random_state: 1234
  tacotron_decay_learning_rate: True
  tacotron_decay_rate: 0.5
  tacotron_decay_steps: 18000
  tacotron_dropout_rate: 0.5
  tacotron_final_learning_rate: 0.0001
  tacotron_initial_learning_rate: 0.001
  tacotron_random_seed: 5339
  tacotron_reg_weight: 1e-06
  tacotron_scale_regularization: True
  tacotron_start_decay: 40000
  tacotron_swap_with_cpu: False
  tacotron_teacher_forcing_decay_alpha: 0.0
  tacotron_teacher_forcing_decay_steps: 40000
  tacotron_teacher_forcing_final_ratio: 0.0
  tacotron_teacher_forcing_init_ratio: 1.0
  tacotron_teacher_forcing_mode: constant
  tacotron_teacher_forcing_ratio: 1.0
  tacotron_teacher_forcing_start_decay: 10000
  tacotron_test_batches: 8
  tacotron_test_size: None
  tacotron_zoneout_rate: 0.1
  train_with_GTA: False
  trim_fft_size: 128
  trim_hop_size: 64
  trim_silence: True
  trim_top_db: 40
  upsample_conditional_features: True
  upsample_scales: [16, 16]
  use_all_gpus: False
  use_bias: True
  use_lws: True
  wavenet_adam_beta1: 0.9
  wavenet_adam_beta2: 0.999
  wavenet_adam_epsilon: 1e-06
  wavenet_batch_size: 4
  wavenet_data_random_state: 1234
  wavenet_dropout: 0.05
  wavenet_ema_decay: 0.9999
  wavenet_learning_rate: 0.0001
  wavenet_random_seed: 5339
  wavenet_swap_with_cpu: False
  wavenet_test_batches: None
  wavenet_test_size: 0.0441
  win_size: 512
[2020-03-10 23:44:22.029]  Loaded metadata for 512 examples (0.43 hours)
[2020-03-10 23:44:35.354]  Initialized Tacotron model. Dimensions (? = dynamic shape): 
[2020-03-10 23:44:35.354]    Train mode:               True
[2020-03-10 23:44:35.354]    Eval mode:                False
[2020-03-10 23:44:35.354]    GTA mode:                 False
[2020-03-10 23:44:35.354]    Synthesis mode:           False
[2020-03-10 23:44:35.354]    embedding:                (?, ?, 512)
[2020-03-10 23:44:35.354]    enc conv out:             (?, ?, 512)
[2020-03-10 23:44:35.354]    encoder out:              (?, ?, 512)
[2020-03-10 23:44:35.354]    decoder out:              (?, ?, 20)
[2020-03-10 23:44:35.354]    residual out:             (?, ?, 512)
[2020-03-10 23:44:35.355]    projected residual out:   (?, ?, 20)
[2020-03-10 23:44:35.355]    mel out:                  (?, ?, 20)
[2020-03-10 23:44:35.355]    <stop_token> out:         (?, ?)
[2020-03-10 23:44:54.239]  Initialized Tacotron model. Dimensions (? = dynamic shape): 
[2020-03-10 23:44:54.239]    Train mode:               False
[2020-03-10 23:44:54.240]    Eval mode:                True
[2020-03-10 23:44:54.240]    GTA mode:                 False
[2020-03-10 23:44:54.240]    Synthesis mode:           False
[2020-03-10 23:44:54.240]    embedding:                (?, ?, 512)
[2020-03-10 23:44:54.240]    enc conv out:             (?, ?, 512)
[2020-03-10 23:44:54.240]    encoder out:              (?, ?, 512)
[2020-03-10 23:44:54.240]    decoder out:              (?, ?, 20)
[2020-03-10 23:44:54.240]    residual out:             (?, ?, 512)
[2020-03-10 23:44:54.240]    projected residual out:   (?, ?, 20)
[2020-03-10 23:44:54.240]    mel out:                  (?, ?, 20)
[2020-03-10 23:44:54.240]    <stop_token> out:         (?, ?)
[2020-03-10 23:44:56.202]  Tacotron training set to a maximum of 500000 steps
[2020-03-10 23:45:00.569]  No model to load at logs-Tacotron/taco_pretrained/
[2020-03-10 23:45:01.087]  
Generated 8 test batches of size 8 in 0.506 sec
[2020-03-10 23:45:12.160]  
Generated 32 train batches of size 8 in 11.578 sec
[2020-03-10 23:45:29.147]  Step       1 [28.576 sec/step, loss=14.65658, avg_loss=14.65658]
[2020-03-10 23:45:46.173]  Step       2 [22.801 sec/step, loss=9.73402, avg_loss=12.19530]
[2020-03-10 23:46:03.939]  Step       3 [21.122 sec/step, loss=10.15927, avg_loss=11.51662]
[2020-03-10 23:46:24.532]  Step       4 [20.990 sec/step, loss=11.16705, avg_loss=11.42923]
[2020-03-10 23:46:37.051]  Step       5 [19.296 sec/step, loss=18.59044, avg_loss=12.86147]
[2020-03-10 23:46:48.575]  Step       6 [18.000 sec/step, loss=10.16290, avg_loss=12.41171]
[2020-03-10 23:47:00.813]  Step       7 [17.177 sec/step, loss=10.71963, avg_loss=12.16998]
[2020-03-10 23:47:34.275]  Step       8 [19.213 sec/step, loss=10.00303, avg_loss=11.89911]
[2020-03-10 23:47:46.716]  Step       9 [18.460 sec/step, loss=7.20985, avg_loss=11.37809]
[2020-03-10 23:48:02.548]  Step      10 [18.197 sec/step, loss=7.29955, avg_loss=10.97023]
[2020-03-10 23:48:28.011]  Step      11 [18.858 sec/step, loss=6.66687, avg_loss=10.57902]
[2020-03-10 23:48:43.618]  Step      12 [18.587 sec/step, loss=7.54546, avg_loss=10.32622]
[2020-03-10 23:49:12.132]  Step      13 [19.351 sec/step, loss=6.59596, avg_loss=10.03928]
